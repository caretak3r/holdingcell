# Example Outputs

## System Check Example

```
Local Cookbook - System Check
Gathering system information...

System Information
============================================================

GPU Information
??????????????????????????????????????????????
? GPU   ? Name                    ? VRAM     ?
??????????????????????????????????????????????
? GPU 0 ? NVIDIA GeForce RTX 3090 ? 24.00 GB ?
? Total ?                         ? 24.00 GB ?
??????????????????????????????????????????????

CPU Information
????????????????????????????????????????????????????????
? Property       ? Value                               ?
????????????????????????????????????????????????????????
? Model          ? AMD Ryzen 9 5950X 16-Core Processor ?
? Physical Cores ? 16                                  ?
? Logical Cores  ? 32                                  ?
? Architecture   ? x86_64                              ?
????????????????????????????????????????????????????????

Memory Information
??????????????????????????????????
? Property      ? Value          ?
??????????????????????????????????
? Total RAM     ? 60.71 GB       ?
? Available RAM ? 57.81 GB       ?
? Used RAM      ? 2.90 GB (4.8%) ?
??????????????????????????????????
```

## Quantized Model Recommendations Example

```
Quantized Model Recommendations
================================================================================
Available VRAM: 24.00 GB
Available RAM: 60.71 GB

Balanced Recommendations:
??????????????????????????????????????????????????????????????????????????????
? Model                ? Size     ? Quant        ? VRAM       ? Quality      ?
??????????????????????????????????????????????????????????????????????????????
? DeepSeek-30B         ? 30B      ? GPTQ-4bit    ? 16.0GB     ? Medium       ?
? DeepSeek-30B         ? 30B      ? AWQ-4bit     ? 16.0GB     ? Medium       ?
? DeepSeek-30B         ? 30B      ? INT4         ? 15.0GB     ? Medium-Low   ?
? Llama-2-13B          ? 13B      ? INT8         ? 13.0GB     ? High-Medium ?
? GLM-4.5V             ? 13B      ? INT8         ? 13.0GB     ? High-Medium ?
??????????????????????????????????????????????????????????????????????????????

Backend-Specific Model Names:
  DeepSeek-30B (GPTQ-4bit):
    ? Ollama: deepseek-coder:33b
    ? vLLM: deepseek-ai/DeepSeek-V3.1
    ? llama.cpp: models/deepseek-30b.gguf

Maximum Model Size You Can Run:
  ? DeepSeek-30B (30B) with GPTQ-4bit quantization
  ? Requires ~16.0 GB VRAM / 19.2 GB RAM

  Pull commands:
    Ollama: ollama pull deepseek-coder:33b
    vLLM: Use model: deepseek-ai/DeepSeek-V3.1
    llama.cpp: Download: models/deepseek-30b.gguf
```

## Benchmark Example

```
LLM Backend Comparison Tool
Benchmarking LLM backends...

Available Backends:
  ? vllm
  ? llama.cpp
  ? ollama

Running benchmarks for DeepSeek-V3.1
Prompt: build the game of life using python...

Running vllm...
  Completed in 12.34s
  Speed: 45.67 tokens/s

Running ollama...
  Completed in 15.67s
  Speed: 32.45 tokens/s

Benchmark Results
================================================================================

Performance Summary
??????????????????????????????????????????????????????????????????????
? Backend  ? Status   ? Time (s)? Tokens/s ? Tokens  ? Output Length ?
??????????????????????????????????????????????????????????????????????
? vllm     ? Success  ? 12.34   ? 45.67    ? 563     ? 2345          ?
? ollama   ? Success  ? 15.67   ? 32.45    ? 508     ? 2156          ?
??????????????????????????????????????????????????????????????????????

Comparison Analysis
================================================================================

Speed Comparison:
  Fastest: vllm (12.34s)
  Slowest: ollama (15.67s)
  Speedup: 1.27x

Token Throughput Comparison:
  Fastest: vllm (45.67 tokens/s)
  Slowest: ollama (32.45 tokens/s)
  Speedup: 1.41x
```

## Common Use Cases

### Finding Maximum Model Size

**Command:**
```bash
uv run python -m local_cookbook.main check
```

**What to Look For:**
- "Maximum Model Size You Can Run" section
- Copy the pull commands provided
- Check VRAM/RAM requirements

### Comparing Backends

**Command:**
```bash
cd ~/local-cookbook/llm-comparisons
uv run python -m llm_comparisons.main benchmark
```

**What to Look For:**
- Speed comparison (fastest backend)
- Token throughput (tokens/second)
- Quality assessment (review outputs)

### Checking Model Compatibility

**Command:**
```bash
uv run python -m local_cookbook.main check --llm glm-4.6 --llm glm-4.5v
```

**What to Look For:**
- Status indicators (? Pass, ? Warning, ? Fail)
- VRAM/RAM requirements vs available
- Warnings section

## Sample Scripts

### Automated System Check

```bash
#!/bin/bash
# check-system.sh

cd ~/local-cookbook
uv run python -m local_cookbook.main check --json > system_report.json
echo "System check complete. Report saved to system_report.json"
```

### Pull Recommended Models

```bash
#!/bin/bash
# pull-models.sh

# Based on system check output, pull recommended models
ollama pull deepseek-coder:33b
ollama pull glm-4v-9b
ollama pull qwen:14b

echo "Models pulled successfully"
```

### Benchmark All Backends

```bash
#!/bin/bash
# benchmark-all.sh

cd ~/local-cookbook/llm-comparisons
uv run python -m llm_comparisons.main benchmark \
  --model qwen-7b \
  --prompt "explain quantum computing in simple terms" \
  --json > benchmark_results.json

echo "Benchmark complete. Results saved to benchmark_results.json"
```
